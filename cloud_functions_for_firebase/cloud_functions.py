# This file was generated by using AI assistant tools.

"""
Cloud Functions for Firebase - Store Import Function
Triggers on JSON file uploads to Cloud Storage and imports store data to Firestore.
"""

import firebase_admin
from firebase_admin import firestore, storage
from firebase_functions import storage_fn
import json
import logging

# Configure logging
logger = logging.getLogger(__name__)

# Initialize Firebase Admin SDK (only once)
if not firebase_admin._apps:
    firebase_admin.initialize_app()

@storage_fn.on_object_finalized(max_instances=1)
def import_stores_json(event: storage_fn.CloudEvent[storage_fn.StorageObjectData]) -> None:
    """
    Cloud Function that triggers on a new file upload to the specified Cloud Storage path,
    downloads it, parses as JSON, and imports/updates store data into Firestore
    in a nested structure: stores/{retailer_id}/branches/{store_id}.
    """
    # Get Firestore client
    db = firestore.client()
    
    # Extract file information from the event
    file_path = event.data.name
    file_bucket_name = event.data.bucket
    content_type = event.data.content_type
    
    logger.info(f"Cloud Function 'import_stores_json' triggered by file: {file_path}")

    if not file_path:
        logger.info("No file name in event, likely an irrelevant update or deletion. Skipping.")
        return
    
    # Only process files in the store-imports/ folder
    if not file_path.startswith("store-imports/"):
        logger.info(f"Skipping file {file_path}. Not in 'store-imports/' folder.")
        return

    if content_type != "application/json":
        logger.warning(f"Skipping file {file_path}. Content-Type is '{content_type}', expected 'application/json'.")
        return

    logger.info(f"Attempting to process JSON file: gs://{file_bucket_name}/{file_path}")

    try:
        # Download and parse the JSON file
        file_bucket = storage.bucket(file_bucket_name)
        blob = file_bucket.blob(file_path)
        json_string = blob.download_as_text()
        store_data_array = json.loads(json_string)

        if not isinstance(store_data_array, list):
            logger.error(
                f"Expected JSON from {file_path} to be a list of store objects, but it was not. "
                f"Parsed type: {type(store_data_array)}"
            )
            return

        # Process stores in batches (Firestore limit: 500 operations per batch)
        BATCH_SIZE = 500
        batch = db.batch()
        stores_processed_count = 0
        operation_count = 0

        for store_item in store_data_array:
            if not isinstance(store_item, dict):
                logger.warning(f"Skipping non-dictionary item in JSON array: {store_item}")
                continue

            # Convert opening_hours to ordered array format
            if "opening_hours" in store_item and isinstance(store_item["opening_hours"], dict):
                days_order = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
                opening_hours_array = []
                for day in days_order:
                    if day in store_item["opening_hours"]:
                        opening_hours_array.append({
                            "day": day,
                            "hours": store_item["opening_hours"][day]
                        })
                store_item["opening_hours"] = opening_hours_array

            # Extract required IDs
            retailer_id = store_item.get("retailer_id")
            store_id = store_item.get("store_id")

            # Validate presence of required IDs
            if not retailer_id:
                logger.warning(f"Store item in {file_path} is missing 'retailer_id'. Skipping: {store_item}")
                continue
            if not store_id:
                logger.warning(f"Store item in {file_path} (retailer: {retailer_id}) is missing 'store_id'. Skipping: {store_item}")
                continue

            # Construct the Nested Firestore Document Reference
            # Path: retailer/{retailer_id}/stores/{store_id}
            doc_ref = db.collection("retailer").document(str(retailer_id)).collection("stores").document(str(store_id))

            # Use merge=False to completely replace the document (deletes removed fields)
            batch.set(doc_ref, store_item, merge=False)
            operation_count += 1
            stores_processed_count += 1

            # Commit batch if we've reached the limit
            if operation_count >= BATCH_SIZE:
                batch.commit()
                logger.info(f"Committed batch of {operation_count} operations.")
                batch = db.batch()
                operation_count = 0

        # Commit any remaining operations
        if operation_count > 0:
            batch.commit()
            logger.info(f"Committed final batch of {operation_count} operations.")

        logger.info(f"Successfully imported/updated {stores_processed_count} store branches "
                    f"from {file_path} into Firestore.")

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse JSON from {file_path}. "
                     f"Please ensure the file is valid JSON: {e}", exc_info=True)
    except Exception as e:
        logger.error(f"An unexpected error occurred while processing {file_path}: {e}", exc_info=True)


@storage_fn.on_object_finalized(max_instances=1)
def import_products_json(event: storage_fn.CloudEvent[storage_fn.StorageObjectData]) -> None:
    """
    Cloud Function that triggers on a new file upload to product-imports/ folder,
    downloads it, parses as JSON, and imports/updates product data into Firestore
    in a flat structure: products/{product_id}.
    """
    # Get Firestore client
    db = firestore.client()
    
    # Extract file information from the event
    file_path = event.data.name
    file_bucket_name = event.data.bucket
    content_type = event.data.content_type
    
    logger.info(f"Cloud Function 'import_products_json' triggered by file: {file_path}")

    if not file_path:
        logger.info("No file name in event, likely an irrelevant update or deletion. Skipping.")
        return
    
    # Only process files in the product-imports/ folder
    if not file_path.startswith("product-imports/"):
        logger.info(f"Skipping file {file_path}. Not in 'product-imports/' folder.")
        return

    if content_type != "application/json":
        logger.warning(f"Skipping file {file_path}. Content-Type is '{content_type}', expected 'application/json'.")
        return

    logger.info(f"Attempting to process JSON file: gs://{file_bucket_name}/{file_path}")

    try:
        # Download and parse the JSON file
        file_bucket = storage.bucket(file_bucket_name)
        blob = file_bucket.blob(file_path)
        json_string = blob.download_as_text()
        product_data_array = json.loads(json_string)

        if not isinstance(product_data_array, list):
            logger.error(
                f"Expected JSON from {file_path} to be a list of product objects, but it was not. "
                f"Parsed type: {type(product_data_array)}"
            )
            return

        # Process products in batches (Firestore limit: 500 operations per batch)
        BATCH_SIZE = 500
        batch = db.batch()
        products_processed_count = 0
        operation_count = 0


        for product_item in product_data_array:
            # Ensure the item is a dictionary
            if not isinstance(product_item, dict):
                logger.warning(f"Skipping non-dictionary item in JSON array: {product_item}")
                continue


            # Extract required ID, name, and product_type
            product_id = product_item.get("product_id")
            # Use 'name' field for product name validation
            name = product_item.get("name")
            # Extract product_type to ensure it's included in Firestore
            product_type = product_item.get("product_type", "")

            # Check for missing or null product_id
            if not product_id:
                logger.warning(f"Product item in {file_path} is missing 'product_id'. Skipping: {product_item}")
                continue

            # Check for null or 'null' name (skip if so)
            # This prevents uploading products that are deleted or not present
            # 'null' string check is case-insensitive and ignores surrounding whitespace
            # isinstance check ensures we don't error on non-string types in name
            if name is None or (isinstance(name, str) and name.strip().lower() == "null"):
                logger.info(f"Skipping product with null name (product_id: {product_id}) from {file_path}.")
                continue

            # Ensure product_type field exists (set to empty string if missing)
            # This field categorizes products (e.g., "milk", "chicken", "tomato bio")
            if "product_type" not in product_item or product_item["product_type"] is None:
                product_item["product_type"] = ""
                logger.info(f"Product {product_id} missing product_type, set to empty string.")

            # Construct the Firestore Document Reference
            # Path: products/{product_id}
            doc_ref = db.collection("products").document(str(product_id))

            # Use merge=False to completely replace the document
            # This includes all fields: product_id, name, category, image_url, 
            # general_unit, product_type, and keywords
            batch.set(doc_ref, product_item, merge=False)
            operation_count += 1
            products_processed_count += 1

            # Commit batch if we've reached the limit
            if operation_count >= BATCH_SIZE:
                batch.commit()
                logger.info(f"Committed batch of {operation_count} operations.")
                batch = db.batch()
                operation_count = 0

        # Commit any remaining operations
        if operation_count > 0:
            batch.commit()
            logger.info(f"Committed final batch of {operation_count} operations.")

        logger.info(f"Successfully imported/updated {products_processed_count} products "
                    f"from {file_path} into Firestore.")

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse JSON from {file_path}. "
                     f"Please ensure the file is valid JSON: {e}", exc_info=True)
    except Exception as e:
        logger.error(f"An unexpected error occurred while processing {file_path}: {e}", exc_info=True)


@storage_fn.on_object_finalized(max_instances=1)
def import_store_prices_json(event: storage_fn.CloudEvent[storage_fn.StorageObjectData]) -> None:
    """
    Cloud Function that triggers on a new file upload to price-imports/ folder,
    downloads it, parses as JSON, and imports/updates store price data into Firestore
    in a flat structure: store_prices/{doc_id} where doc_id = store_id + product_id.
    
    Note: This function removes duplicate product metadata (product_name, product_image, category)
    since that data belongs in the products collection.
    """
    # Get Firestore client
    db = firestore.client()
    
    # Extract file information from the event
    file_path = event.data.name
    file_bucket_name = event.data.bucket
    content_type = event.data.content_type
    
    logger.info(f"Cloud Function 'import_store_prices_json' triggered by file: {file_path}")

    if not file_path:
        logger.info("No file name in event, likely an irrelevant update or deletion. Skipping.")
        return
    
    # Only process files in the price-imports/ folder
    if not file_path.startswith("price-imports/"):
        logger.info(f"Skipping file {file_path}. Not in 'price-imports/' folder.")
        return

    if content_type != "application/json":
        logger.warning(f"Skipping file {file_path}. Content-Type is '{content_type}', expected 'application/json'.")
        return

    logger.info(f"Attempting to process JSON file: gs://{file_bucket_name}/{file_path}")

    try:
        # Download and parse the JSON file
        file_bucket = storage.bucket(file_bucket_name)
        blob = file_bucket.blob(file_path)
        json_string = blob.download_as_text()
        price_data_array = json.loads(json_string)

        if not isinstance(price_data_array, list):
            logger.error(
                f"Expected JSON from {file_path} to be a list of price objects, but it was not. "
                f"Parsed type: {type(price_data_array)}"
            )
            return

        # Process prices in batches (Firestore limit: 500 operations per batch)
        BATCH_SIZE = 500
        batch = db.batch()
        prices_processed_count = 0
        operation_count = 0

        # Fields to remove (duplicate data that belongs in products collection)
        duplicate_fields = ["product_name", "product_image", "category"]

        for price_item in price_data_array:
            # Ensure the item is a dictionary
            if not isinstance(price_item, dict):
                logger.warning(f"Skipping non-dictionary item in JSON array: {price_item}")
                continue

            # Extract required IDs and product_name
            doc_id = price_item.get("doc_id")
            store_id = price_item.get("store_id")
            product_id = price_item.get("product_id")
            product_name = price_item.get("product_name")

            # Validate presence of required IDs
            if not doc_id:
                logger.warning(f"Price item in {file_path} is missing 'doc_id'. Skipping: {price_item}")
                continue
            if not store_id:
                logger.warning(f"Price item in {file_path} is missing 'store_id'. Skipping: {price_item}")
                continue
            if not product_id:
                logger.warning(f"Price item in {file_path} is missing 'product_id'. Skipping: {price_item}")
                continue

            # Check for null or 'null' product_name (skip if so)
            # This prevents uploading price items for products that are deleted or not present
            # 'null' string check is case-insensitive and ignores surrounding whitespace
            # isinstance check ensures we don't error on non-string types in product_name
            if product_name is None or (isinstance(product_name, str) and product_name.strip().lower() == "null"):
                logger.info(f"Skipping store price with null product_name (product_id: {product_id}, doc_id: {doc_id}) from {file_path}.")
                continue

            # Remove duplicate product metadata fields
            for field in duplicate_fields:
                price_item.pop(field, None)

            # Construct the Firestore Document Reference
            # Path: store_prices/{doc_id}
            doc_ref = db.collection("store_prices").document(str(doc_id))

            # Use merge=False to completely replace the document
            batch.set(doc_ref, price_item, merge=False)
            operation_count += 1
            prices_processed_count += 1

            # Commit batch if we've reached the limit
            if operation_count >= BATCH_SIZE:
                batch.commit()
                logger.info(f"Committed batch of {operation_count} operations.")
                batch = db.batch()
                operation_count = 0

        # Commit any remaining operations
        if operation_count > 0:
            batch.commit()
            logger.info(f"Committed final batch of {operation_count} operations.")

        logger.info(f"Successfully imported/updated {prices_processed_count} store prices "
                    f"from {file_path} into Firestore.")

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse JSON from {file_path}. "
                     f"Please ensure the file is valid JSON: {e}", exc_info=True)
    except Exception as e:
        logger.error(f"An unexpected error occurred while processing {file_path}: {e}", exc_info=True)


@storage_fn.on_object_finalized(max_instances=1)
def import_product_types_json(event: storage_fn.CloudEvent[storage_fn.StorageObjectData]) -> None:
    """
    Cloud Function that triggers on a new file upload to type-imports/ folder,
    downloads it, parses as JSON, and imports/updates product type data into Firestore
    in a flat structure: product_types/{product_type}.
    
    Product types serve as the search/discovery layer, with images and metadata
    that allow users to select a product category before optimization.
    """
    # Get Firestore client
    db = firestore.client()
    
    # Extract file information from the event
    file_path = event.data.name
    file_bucket_name = event.data.bucket
    content_type = event.data.content_type
    
    logger.info(f"Cloud Function 'import_product_types_json' triggered by file: {file_path}")

    if not file_path:
        logger.info("No file name in event, likely an irrelevant update or deletion. Skipping.")
        return
    
    # Only process files in the type-imports/ folder
    if not file_path.startswith("type-imports/"):
        logger.info(f"Skipping file {file_path}. Not in 'type-imports/' folder.")
        return

    if content_type != "application/json":
        logger.warning(f"Skipping file {file_path}. Content-Type is '{content_type}', expected 'application/json'.")
        return

    logger.info(f"Attempting to process JSON file: gs://{file_bucket_name}/{file_path}")

    try:
        # Download and parse the JSON file
        file_bucket = storage.bucket(file_bucket_name)
        blob = file_bucket.blob(file_path)
        json_string = blob.download_as_text()
        type_data_array = json.loads(json_string)

        if not isinstance(type_data_array, list):
            logger.error(
                f"Expected JSON from {file_path} to be a list of product type objects, but it was not. "
                f"Parsed type: {type(type_data_array)}"
            )
            return

        # Process product types in batches (Firestore limit: 500 operations per batch)
        BATCH_SIZE = 500
        batch = db.batch()
        types_processed_count = 0
        operation_count = 0

        for type_item in type_data_array:
            # Ensure the item is a dictionary
            if not isinstance(type_item, dict):
                logger.warning(f"Skipping non-dictionary item in JSON array: {type_item}")
                continue

            # Extract required product_type field
            product_type = type_item.get("product_type")

            # Check for missing product_type
            if not product_type:
                logger.warning(f"Product type item in {file_path} is missing 'product_type'. Skipping: {type_item}")
                continue

            # Construct the Firestore Document Reference
            # Path: product_types/{product_type}
            doc_ref = db.collection("product_types").document(str(product_type))

            # Use merge=False to completely replace the document
            # This includes all fields: product_type, display_name, image_url, category,
            # search_keywords, unit, and any other metadata
            batch.set(doc_ref, type_item, merge=False)
            operation_count += 1
            types_processed_count += 1

            # Commit batch if we've reached the limit
            if operation_count >= BATCH_SIZE:
                batch.commit()
                logger.info(f"Committed batch of {operation_count} operations.")
                batch = db.batch()
                operation_count = 0

        # Commit any remaining operations
        if operation_count > 0:
            batch.commit()
            logger.info(f"Committed final batch of {operation_count} operations.")

        logger.info(f"Successfully imported/updated {types_processed_count} product types "
                    f"from {file_path} into Firestore.")

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse JSON from {file_path}. "
                     f"Please ensure the file is valid JSON: {e}", exc_info=True)
    except Exception as e:
        logger.error(f"An unexpected error occurred while processing {file_path}: {e}", exc_info=True)


@storage_fn.on_object_finalized(max_instances=1)
def import_retailer_prices_json(event: storage_fn.CloudEvent[storage_fn.StorageObjectData]) -> None:
    """
    Cloud Function that triggers on a new file upload to retailer-prices-imports/ folder,
    downloads it, parses as JSON, and imports/updates retailer price data into Firestore
    in a nested structure: retailer/{retailer_id}/products_per_retailer/{doc_id}.
    
    This function stores product and price data as a subcollection under each retailer, allowing
    for organized product/price management per retailer.
    """
    # Get Firestore client
    db = firestore.client()
    
    # Extract file information from the event
    file_path = event.data.name
    file_bucket_name = event.data.bucket
    content_type = event.data.content_type
    
    logger.info(f"Cloud Function 'import_retailer_prices_json' triggered by file: {file_path}")

    if not file_path:
        logger.info("No file name in event, likely an irrelevant update or deletion. Skipping.")
        return
    
    # Only process files in the retailer-prices-imports/ folder
    if not file_path.startswith("retailer-prices-imports/"):
        logger.info(f"Skipping file {file_path}. Not in 'retailer-prices-imports/' folder.")
        return

    if content_type != "application/json":
        logger.warning(f"Skipping file {file_path}. Content-Type is '{content_type}', expected 'application/json'.")
        return

    logger.info(f"Attempting to process JSON file: gs://{file_bucket_name}/{file_path}")

    try:
        # Download and parse the JSON file
        file_bucket = storage.bucket(file_bucket_name)
        blob = file_bucket.blob(file_path)
        json_string = blob.download_as_text()
        price_data_array = json.loads(json_string)

        if not isinstance(price_data_array, list):
            logger.error(
                f"Expected JSON from {file_path} to be a list of price objects, but it was not. "
                f"Parsed type: {type(price_data_array)}"
            )
            return

        # Process prices in batches (Firestore limit: 500 operations per batch)
        BATCH_SIZE = 500
        batch = db.batch()
        prices_processed_count = 0
        operation_count = 0

        for price_item in price_data_array:
            # Ensure the item is a dictionary
            if not isinstance(price_item, dict):
                logger.warning(f"Skipping non-dictionary item in JSON array: {price_item}")
                continue

            # Extract required IDs
            retailer_id = price_item.get("retailer_id")
            doc_id = price_item.get("doc_id")
            product_id = price_item.get("product_id")
            price = price_item.get("price")
            product_name = price_item.get("product_name")

            # Validate presence of required IDs
            if not retailer_id:
                logger.warning(f"Price item in {file_path} is missing 'retailer_id'. Skipping: {price_item}")
                continue
            if not doc_id:
                logger.warning(f"Price item in {file_path} is missing 'doc_id'. Skipping: {price_item}")
                continue
            if not product_id:
                logger.warning(f"Price item in {file_path} is missing 'product_id'. Skipping: {price_item}")
                continue

            # Check for null or 'null' product_name (skip if so)
            # This prevents uploading price items for products that are deleted or not present
            if product_name is None or (isinstance(product_name, str) and product_name.strip().lower() == "null"):
                logger.info(f"Skipping retailer price with null product_name (product_id: {product_id}, doc_id: {doc_id}) from {file_path}.")
                continue

            # Check for null price
            if price is None:
                logger.info(f"Skipping retailer price with null price (product_id: {product_id}, doc_id: {doc_id}) from {file_path}.")
                continue

            # Remove discount_active field as it's redundant (presence of has_discount indicates active discount)
            price_item.pop("discount_active", None)

            # Construct the Firestore Document Reference
            # Path: retailer/{retailer_id}/products_per_retailer/{doc_id}
            doc_ref = db.collection("retailer").document(str(retailer_id)).collection("products_per_retailer").document(str(doc_id))

            # Use merge=False to completely replace the document
            batch.set(doc_ref, price_item, merge=False)
            operation_count += 1
            prices_processed_count += 1

            # Commit batch if we've reached the limit
            if operation_count >= BATCH_SIZE:
                batch.commit()
                logger.info(f"Committed batch of {operation_count} operations.")
                batch = db.batch()
                operation_count = 0

        # Commit any remaining operations
        if operation_count > 0:
            batch.commit()
            logger.info(f"Committed final batch of {operation_count} operations.")

        logger.info(f"Successfully imported/updated {prices_processed_count} retailer prices "
                    f"from {file_path} into Firestore.")

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse JSON from {file_path}. "
                     f"Please ensure the file is valid JSON: {e}", exc_info=True)
    except Exception as e:
        logger.error(f"An unexpected error occurred while processing {file_path}: {e}", exc_info=True)


@storage_fn.on_object_finalized(max_instances=1)
def import_discounts_json(event: storage_fn.CloudEvent[storage_fn.StorageObjectData]) -> None:
    """
    Cloud Function that triggers on a new file upload to discount-imports/ folder,
    downloads it, parses as JSON, and imports/updates discount data into Firestore.
    
    Process flow:
    1. Validates file is in discount-imports/ folder and is valid JSON
    2. Creates/updates documents in discounts/{product_id} collection with full discount details
    3. Updates ALL products_per_retailer documents for that product_id with a has_discount nested object
       containing: discounted_price, discount_percentage, discount_start, discount_end
    4. Uses batching to handle Firestore's 500 operation limit per batch write
    
    Data structure created in products_per_retailer:
    {
        "retailer_id": "...",
        "product_id": "...",
        "price": 5.70,  # Base price (unchanged)
        "has_discount": {
            "discounted_price": 4.56,
            "discount_percentage": 20,
            "discount_start": "2026-01-10T00:00:00Z",
            "discount_end": "2026-01-10T00:00:00Z"
        }
    }
    
    Note: discount_active is NOT included in has_discount because the presence of the has_discount
    field itself indicates an active discount. The full discount record (including historical status)
    is stored in the discounts/{product_id} collection.
    """
    # Initialize Firestore client for database operations
    db = firestore.client()
    
    # Extract event data: file name, bucket location, and content type
    file_path = event.data.name
    file_bucket_name = event.data.bucket
    content_type = event.data.content_type
    
    logger.info(f"Cloud Function 'import_discounts_json' triggered by file: {file_path}")

    # Skip if no file name (likely a metadata-only update or deletion event)
    if not file_path:
        logger.info("No file name in event, likely an irrelevant update or deletion. Skipping.")
        return
    
    # FILTER 1: Only process files in the discount-imports/ folder
    # This prevents the function from triggering on unrelated storage events
    if not file_path.startswith("discount-imports/"):
        logger.info(f"Skipping file {file_path}. Not in 'discount-imports/' folder.")
        return

    # FILTER 2: Validate file is JSON format
    # Only JSON files should be processed; other formats are skipped with a warning
    if content_type != "application/json":
        logger.warning(f"Skipping file {file_path}. Content-Type is '{content_type}', expected 'application/json'.")
        return

    logger.info(f"Attempting to process JSON file: gs://{file_bucket_name}/{file_path}")

    try:
        # ==================== DOWNLOAD AND PARSE JSON ====================
        # Download the JSON file from Cloud Storage and parse it into a Python list
        file_bucket = storage.bucket(file_bucket_name)
        blob = file_bucket.blob(file_path)
        json_string = blob.download_as_text()
        discount_data_array = json.loads(json_string)

        # Validate that the JSON is a list (array) of discount objects
        # Expected format: [{ "product_id": "123", "discounted_price": 4.56, ... }, ...]
        if not isinstance(discount_data_array, list):
            logger.error(
                f"Expected JSON from {file_path} to be a list of discount objects, but it was not. "
                f"Parsed type: {type(discount_data_array)}"
            )
            return

        # ==================== BATCH PROCESSING ====================
        # Firestore has a limit of 500 operations per batch commit
        # We accumulate write operations and commit when reaching the limit
        BATCH_SIZE = 500
        batch = db.batch()
        discounts_processed_count = 0  # Track total successful discounts processed
        operation_count = 0  # Track operations in current batch
        
        logger.info(f"Starting to process {len(discount_data_array)} discounts...")

        # ==================== ITERATE THROUGH EACH DISCOUNT ====================
        for idx, discount_item in enumerate(discount_data_array, 1):
            # Ensure each item is a dictionary (not a string, number, etc.)
            if not isinstance(discount_item, dict):
                logger.warning(f"Skipping non-dictionary item in JSON array: {discount_item}")
                continue

            # ==================== VALIDATE REQUIRED FIELDS ====================
            # Extract product_id (used as document identifier and for store_prices lookup)
            product_id = discount_item.get("product_id")

            # Validate product_id is present and not None
            # Without product_id, we can't link discount to products or store prices
            if not product_id:
                logger.warning(f"Discount item in {file_path} is missing 'product_id'. Skipping: {discount_item}")
                continue

            # ==================== STORE IN DISCOUNTS COLLECTION ====================
            # Path: discounts/{product_id}
            # This is the source of truth for discount data
            # merge=False means this completely replaces the document (deletes old fields)
            discount_doc_ref = db.collection("discounts").document(str(product_id))
            batch.set(discount_doc_ref, discount_item, merge=False)
            operation_count += 1

            # ==================== PREPARE DISCOUNT OBJECT FOR PRODUCTS_PER_RETAILER ====================
            # Create a nested object with discount details to embed in products_per_retailer documents
            # Note: We exclude product_id and other redundant fields that are already in products_per_retailer
            # Note: We DON'T include base_price since that's already stored in the 'price' field
            # Note: We DON'T include discount_active since the presence of has_discount itself
            #       indicates this product has a discount. Clients can check validity using date ranges.
            has_discount_obj = {
                "discounted_price": discount_item.get("discounted_price"),
                "discount_percentage": discount_item.get("discount_percentage"),
                "discount_start": discount_item.get("discount_start"),
                "discount_end": discount_item.get("discount_end")
            }

            # ==================== UPDATE ALL MATCHING PRODUCTS_PER_RETAILER DOCUMENTS ====================
            # doc_id format is: {retailer_id}_{product_id}
            # We can extract retailer_id from doc_id
            logger.info(f"[Discount {idx}] Processing product_id '{product_id}'")
            
            product_updated_count = 0
            
            try:
                # List of known retailers
                retailer_ids = ["aldi", "aligro", "coop", "denner", "migros", "other"]
                
                for retailer_id in retailer_ids:
                    # Construct the doc_id that would exist for this retailer + product
                    doc_id = f"{retailer_id}_{product_id}"
                    
                    # Direct path to the document
                    doc_ref = db.collection("retailer").document(retailer_id).collection("products_per_retailer").document(doc_id)
                    
                    try:
                        # Try to get the document
                        doc = doc_ref.get()
                        if doc.exists:
                            logger.info(f"[Discount {idx}] Found {doc_id} in {retailer_id}, updating...")
                            batch.update(doc_ref, {"has_discount": has_discount_obj})
                            operation_count += 1
                            product_updated_count += 1
                            
                            if operation_count >= BATCH_SIZE:
                                batch.commit()
                                logger.info(f"Committed batch of {operation_count} operations.")
                                batch = db.batch()
                                operation_count = 0
                        else:
                            logger.info(f"[Discount {idx}] Document {doc_id} does not exist in {retailer_id}")
                    except Exception as e:
                        logger.error(f"[Discount {idx}] Error updating {doc_id} in {retailer_id}: {e}")
                        continue
                
            except Exception as e:
                logger.error(f"[Discount {idx}] Error during update: {e}", exc_info=True)
            
            logger.info(f"[Discount {idx}] Completed: Updated {product_updated_count} products")

            discounts_processed_count += 1

        # ==================== FINAL BATCH COMMIT ====================
        # Commit any remaining operations that didn't trigger the batch size limit
        if operation_count > 0:
            batch.commit()
            logger.info(f"Committed final batch of {operation_count} operations.")

        # Log successful completion with count of discounts processed
        logger.info(f"Successfully imported/updated {discounts_processed_count} discounts "
                    f"from {file_path} into Firestore.")

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse JSON from {file_path}. "
                     f"Please ensure the file is valid JSON: {e}", exc_info=True)
    except Exception as e:
        logger.error(f"An unexpected error occurred while processing {file_path}: {e}", exc_info=True)

